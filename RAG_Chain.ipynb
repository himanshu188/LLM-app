{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Python libraries"
      ],
      "metadata": {
        "id": "tMUntSPuEKIJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3edUrSn8Q1zj"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes transformers sentence-transformers faiss-gpu\n",
        "!pip install -q langchain langchain-community pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Locale if running in Google Colab"
      ],
      "metadata": {
        "id": "VyQH-scrEO09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If running in Google Colab, you may need to run this cell to make sure you're using UTF-8 locale to install LangChain\n",
        "import locale\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "DxFC2uyqRnmR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the NVIDIA Q1-2025 10-Q report and store it in document.pdf"
      ],
      "metadata": {
        "id": "yPLGWMYFETd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: download pdf file and store it in doc variable\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://s201.q4cdn.com/141608511/files/doc_financials/2025/q1/NVIDIA-10Q-20242905.pdf\"\n",
        "response = requests.get(url)\n",
        "\n",
        "with open(\"document.pdf\", \"wb\") as f:\n",
        "  f.write(response.content)\n",
        "\n",
        "doc = \"document.pdf\"\n"
      ],
      "metadata": {
        "id": "WM_4LWNcRr5H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, Load the 10-Q report using PyMuPDF Loader"
      ],
      "metadata": {
        "id": "45JuoLyAEam3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: langchain load pdf file\n",
        "\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(\"document.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Print the text of the first page\n",
        "print(docs[0].page_content)\n"
      ],
      "metadata": {
        "id": "87js3UWWUBuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunk the documents with some overlap of 30 characters"
      ],
      "metadata": {
        "id": "sZ0z3zqZEhLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
        "\n",
        "chunked_docs = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "AH5gvDiQSgdI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now load the chunks into vector DB FAISS from langchain"
      ],
      "metadata": {
        "id": "0XAQMyScElcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\"))"
      ],
      "metadata": {
        "id": "7csnd9f_TiHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now create retriever from the vector DB"
      ],
      "metadata": {
        "id": "2dpGgl0CEr0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "DxUryl7-Ue0j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the Torch and transformers and load the Zephyr Model with quantization for 4-bit"
      ],
      "metadata": {
        "id": "61R442xhErD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "v89mtJC2UjrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a HF pipeline for this model and create chain of operations"
      ],
      "metadata": {
        "id": "KgeFrxJDE7-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.2,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=400,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "<|system|>\n",
        "Answer the question based on your knowledge. Use the following context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "</s>\n",
        "<|user|>\n",
        "{question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\n",
        " \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "llm_chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "u1kymW4zUwui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now create RAG chain of operation with retriever for the external data source"
      ],
      "metadata": {
        "id": "YIY_PgVbFRzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain"
      ],
      "metadata": {
        "id": "FQWZFVNwVeuK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Type your question here and get the output from the RAG chain"
      ],
      "metadata": {
        "id": "2KI24r4AFX8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What's the EPS, Asset and Liababilities for this quarter for NVDA?\"\n",
        "output = rag_chain.invoke(question)"
      ],
      "metadata": {
        "id": "gGL3DNzNVohh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display the output with doing regex to lookup the right format"
      ],
      "metadata": {
        "id": "Q3Hbp_xMFb73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: strip everything before <|assistant|> in output\n",
        "\n",
        "import re\n",
        "\n",
        "def strip_before_assistant(output):\n",
        "  \"\"\"Strips everything before <|assistant|> in the output string.\n",
        "\n",
        "  Args:\n",
        "    output: The output string.\n",
        "\n",
        "  Returns:\n",
        "    The output string with everything before <|assistant|> removed.\n",
        "  \"\"\"\n",
        "\n",
        "  # Use a regular expression to find the first occurrence of <|assistant|>\n",
        "  match = re.search(r\"<\\|assistant\\|>\", output)\n",
        "\n",
        "  # If the pattern is found, return the substring after the match\n",
        "  if match:\n",
        "    return output[match.end():]\n",
        "\n",
        "  # Otherwise, return the original string\n",
        "  return output\n",
        "\n",
        "stripped_output = strip_before_assistant(output)\n",
        "\n",
        "print(stripped_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT99u5ZlZKxt",
        "outputId": "8129d38a-7ae2-4702-b88c-fb0d5c3dc344"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "  Based on the provided context, here is the information you need:\n",
            "\n",
            "- EPS (earnings per share): The condensed consolidated statements of income provided in the document show that NVIDIA reported net income of $14,881 million for the three months ended April 28, 2024. Assuming no new shares were issued during this time, dividing this net income by the weighted average number of common shares outstanding would give us the earnings per share (EPS). However, the document does not provide the weighted average number of common shares outstanding for this quarter. You may need to refer to previous filings or contact the company directly for this information.\n",
            "\n",
            "- Assets: According to the condensed consolidated balance sheets provided, as of April 28, 2024, NVIDIA had total assets of $53,729 million.\n",
            "\n",
            "- Liabilities: According to the notes to condensed consolidated financial statements, as of April 28, 2024, NVIDIA had accrued and other current liabilities of $11,565 million, which includes items such as taxes payable, customer program accruals, excess inventory purchase obligations, deferred revenue, product warranty and return provisions, accrued payroll and related expenses, unsettled share repurchases, and operating leases. Additionally, the company had long-term debt of $5,632 million as of April 28, 2024. When adding up all liabilities (both current and long-term), we get a total liability amount of $21,197 million as of April 28, 2024.\n",
            "\n",
            "Note: It's important to remember that these figures are condensed and unaudited, and should be verified against the full annual report or audited\n"
          ]
        }
      ]
    }
  ]
}